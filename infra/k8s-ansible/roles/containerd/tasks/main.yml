# --------------------
# Containerd + Kubernetes + Flannel
# --------------------

# 1️⃣ Installer containerd et CRI
- name: Supprimer ancienne config containerd si présente
  file:
    path: /etc/containerd/config.toml
    state: absent
  become: yes
  tags: containerd_install

# ... toutes les tâches containerd/crictl existantes ici ...
# Télécharger, extraire, configurer containerd
# Vérifier que CRI répond

# 2️⃣ Initialiser Kubernetes
- name: Vérifier si le cluster Kubernetes est déjà initialisé
  stat:
    path: /etc/kubernetes/admin.conf
  register: kubeadm_config
  become: yes
  tags: kubernetes_init

- name: Initialize Kubernetes cluster (master only)
  ansible.builtin.command: >
    kubeadm init
    --pod-network-cidr={{ pod_network_cidr }}
    --control-plane-endpoint={{ hostvars[groups['k8s_masters'][0]].ansible_host }}
    --upload-certs
  args:
    creates: /etc/kubernetes/admin.conf
  when:
    - inventory_hostname == groups['k8s_masters'][0]
    - not kubeadm_config.stat.exists
  register: kubeadm_init
  notify:
    - Save kubeconfig file
    - Configure kubectl
  tags: kubernetes_init

# 3️⃣ Appliquer Flannel CNI
- name: Apply Flannel CNI manifest
  ansible.builtin.command:
    cmd: "kubectl apply -f {{ flannel_manifest_url }}"
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: flannel_apply
  changed_when: flannel_apply.rc == 0
  tags: flannel

- name: Wait for Flannel pods to be Running
  ansible.builtin.shell: >
    kubectl get pods -n {{ flannel_namespace }} -l {{ flannel_label_selector }} -o jsonpath='{.items[*].status.phase}' | grep -v Running || true
  register: flannel_status
  retries: "{{ flannel_wait_retries }}"
  delay: "{{ flannel_wait_delay }}"
  until: "'Pending' not in flannel_status.stdout and 'CrashLoopBackOff' not in flannel_status.stdout"
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  changed_when: false
  failed_when: false
  tags: flannel

# 4️⃣ Configurer kube-proxy RBAC après Flannel
- name: Ensure kube-proxy RBAC is configured
  become: yes
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: system:node-proxier
      rules:
        - apiGroups: [""]
          resources:
            - endpoints
            - services
            - nodes
          verbs: ["get", "list", "watch"]
        - apiGroups: ["discovery.k8s.io"]
          resources:
            - endpointslices
          verbs: ["get", "list", "watch"]
        - apiGroups: ["events.k8s.io"]
          resources:
            - events
          verbs: ["create", "patch", "update"]

- name: Ensure kube-proxy ClusterRoleBinding exists
  become: yes
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: system:kube-proxy
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: system:node-proxier
      subjects:
        - kind: ServiceAccount
          name: kube-proxy
          namespace: kube-system

- name: Restart kube-proxy DaemonSet
  become: yes
  kubernetes.core.k8s:
    state: patched
    definition:
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: kube-proxy
        namespace: kube-system

